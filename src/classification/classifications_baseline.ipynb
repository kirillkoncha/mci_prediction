{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQXKlctI8UYv",
    "outputId": "2582d890-acf8-46a1-f031-df2887ccd7f8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define custom scorers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m scoring \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m: make_scorer(f1_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision_macro\u001b[39m\u001b[38;5;124m'\u001b[39m: make_scorer(precision_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall_macro\u001b[39m\u001b[38;5;124m'\u001b[39m: make_scorer(recall_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_name, task_df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtasks\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     task_df \u001b[38;5;241m=\u001b[39m task_df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Clean up\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tasks' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "# Define custom scorers\n",
    "scoring = {\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro')\n",
    "}\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    print(f\"\\n### {task_name} ###\\n\")\n",
    "    task_df = task_df.dropna(subset=['speech', 'mmse', 'diagnosis'])  # Clean up\n",
    "    X = task_df[['speech', 'mmse']]\n",
    "    y = LabelEncoder().fit_transform(task_df['diagnosis'])\n",
    "\n",
    "    # Define column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(max_features=5000, stop_words=stop_words), 'speech'),\n",
    "            ('mmse', StandardScaler(), ['mmse'])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        cv_results = cross_validate(pipeline, X, y, cv=kf, scoring=scoring)\n",
    "\n",
    "        print(f\"F1-Macro: {cv_results['test_f1_macro'].mean():.4f} Â± {cv_results['test_f1_macro'].std():.4f}\")\n",
    "        print(f\"Precision-Macro: {cv_results['test_precision_macro'].mean():.4f}\")\n",
    "        print(f\"Recall-Macro: {cv_results['test_recall_macro'].mean():.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean F1 Macro\": cv_results['test_f1_macro'].mean(),\n",
    "            \"Mean Precision Macro\": cv_results['test_precision_macro'].mean(),\n",
    "            \"Mean Recall Macro\": cv_results['test_recall_macro'].mean(),\n",
    "            \"Std F1\": cv_results['test_f1_macro'].std(),\n",
    "            \"Type\": \"TF-IDF + MMSE\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MCI vs. AD ###\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n    result = self._call_func_on_transformers(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n    return Parallel(n_jobs=self.n_jobs)(jobs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"wasn't\", 'any', 'his', \"aren't\", 'i', 'her', \"hadn't\", 's', 'or', 'have', 'he', 'which', 'yourselves', \"he'd\", 'd', 'again', 'himself', 'our', \"she's\", 'at', \"didn't\", \"i'd\", 'through', \"we'll\", 'while', 'few', 'been', 'being', \"it'd\", 'once', 'this', 'am', 'what', 'no', 'own', 'a', 'll', 'herself', \"won't\", 'off', 'those', 'same', 'ours', 'now', 'into', 'during', 'after', 'is', 'other', 'out', 'didn', 'doesn', 'between', 'further', 'that', 'themselves', 'had', 'over', \"isn't\", 'by', 'from', 'too', \"haven't\", 'not', 'can', 'ourselves', 'we', 'wouldn', \"mustn't\", \"needn't\", 'these', 'ma', 'both', 'up', \"we've\", 'won', \"it's\", 'needn', \"wouldn't\", 'me', \"they'd\", 'so', 'hasn', 'about', 'shan', 'where', 'but', 'if', 'before', \"i'm\", 'yourself', 'doing', 'such', 'than', 'y', 'will', 'was', 'why', 'they', \"don't\", 'because', \"doesn't\", \"couldn't\", 'them', 'having', 'all', 'don', \"i've\", 't', \"we'd\", 'below', 'you', 'for', 'ain', 'weren', 'in', 'then', 'my', \"he'll\", 'hers', 'it', 'when', 'haven', \"it'll\", 'only', 'be', \"hasn't\", 'm', 'each', \"he's\", \"should've\", 'were', \"they're\", 'who', 'him', 'most', \"weren't\", 'the', 've', 'hadn', 'under', \"she'll\", 'on', 'until', 'of', 'mightn', 'did', 'shouldn', 'isn', \"you're\", 'mustn', \"they'll\", \"shan't\", 'whom', \"you'd\", 'here', 'there', 'does', 'she', 'its', 'do', 'down', \"i'll\", \"mightn't\", 'are', 'just', 'as', 'against', 'and', 'nor', 'how', 'with', 'above', \"shouldn't\", 'your', 'their', \"they've\", \"that'll\", \"she'd\", 'has', 'wasn', 'more', \"you'll\", 'myself', 'o', \"you've\", 'very', 'should', 'an', 'couldn', 'itself', 'to', 'theirs', \"we're\", 'some', 're', 'yours', 'aren'} instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m baseline_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     59\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, baseline_preprocessor),\n\u001b[1;32m     60\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     61\u001b[0m ])\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Baseline: TF-IDF only\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_scorer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline (TF-IDF only) F1-Macro: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbaseline_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:431\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    411\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    412\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    429\u001b[0m )\n\u001b[0;32m--> 431\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 1001, in fit_transform\n    result = self._call_func_on_transformers(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 910, in _call_func_on_transformers\n    return Parallel(n_jobs=self.n_jobs)(jobs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {\"wasn't\", 'any', 'his', \"aren't\", 'i', 'her', \"hadn't\", 's', 'or', 'have', 'he', 'which', 'yourselves', \"he'd\", 'd', 'again', 'himself', 'our', \"she's\", 'at', \"didn't\", \"i'd\", 'through', \"we'll\", 'while', 'few', 'been', 'being', \"it'd\", 'once', 'this', 'am', 'what', 'no', 'own', 'a', 'll', 'herself', \"won't\", 'off', 'those', 'same', 'ours', 'now', 'into', 'during', 'after', 'is', 'other', 'out', 'didn', 'doesn', 'between', 'further', 'that', 'themselves', 'had', 'over', \"isn't\", 'by', 'from', 'too', \"haven't\", 'not', 'can', 'ourselves', 'we', 'wouldn', \"mustn't\", \"needn't\", 'these', 'ma', 'both', 'up', \"we've\", 'won', \"it's\", 'needn', \"wouldn't\", 'me', \"they'd\", 'so', 'hasn', 'about', 'shan', 'where', 'but', 'if', 'before', \"i'm\", 'yourself', 'doing', 'such', 'than', 'y', 'will', 'was', 'why', 'they', \"don't\", 'because', \"doesn't\", \"couldn't\", 'them', 'having', 'all', 'don', \"i've\", 't', \"we'd\", 'below', 'you', 'for', 'ain', 'weren', 'in', 'then', 'my', \"he'll\", 'hers', 'it', 'when', 'haven', \"it'll\", 'only', 'be', \"hasn't\", 'm', 'each', \"he's\", \"should've\", 'were', \"they're\", 'who', 'him', 'most', \"weren't\", 'the', 've', 'hadn', 'under', \"she'll\", 'on', 'until', 'of', 'mightn', 'did', 'shouldn', 'isn', \"you're\", 'mustn', \"they'll\", \"shan't\", 'whom', \"you'd\", 'here', 'there', 'does', 'she', 'its', 'do', 'down', \"i'll\", \"mightn't\", 'are', 'just', 'as', 'against', 'and', 'nor', 'how', 'with', 'above', \"shouldn't\", 'your', 'their', \"they've\", \"that'll\", \"she'd\", 'has', 'wasn', 'more', \"you'll\", 'myself', 'o', \"you've\", 'very', 'should', 'an', 'couldn', 'itself', 'to', 'theirs', \"we're\", 'some', 're', 'yours', 'aren'} instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load your data\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "df = result  # Assuming 'result' is already your DataFrame\n",
    "\n",
    "# Define classification tasks\n",
    "tasks = {\n",
    "    \"MCI vs. AD\": df[df['diagnosis'].isin(['MCI', 'AD'])],\n",
    "    \"MCI vs. Control\": df[df['diagnosis'].isin(['MCI', 'Control'])],\n",
    "    \"AD vs. Control\": df[df['diagnosis'].isin(['AD', 'Control'])],\n",
    "    \"MCI vs. AD vs. Control\": df\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"SVM\": SVC(random_state=42, class_weight='balanced'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"mmse\")  # mmse will be added manually at the start\n",
    "numerical_features.insert(0, \"mmse\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    print(f\"\\n### {task_name} ###\\n\")\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        X_text = task_df[['speech']]  # Start with text only\n",
    "        y = LabelEncoder().fit_transform(task_df['diagnosis'])\n",
    "\n",
    "        # Define TF-IDF for baseline\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, 1), max_features=5000, stop_words=stop_words)\n",
    "\n",
    "        baseline_preprocessor = ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech')\n",
    "        ])\n",
    "\n",
    "        baseline_pipeline = Pipeline([\n",
    "            ('preprocessor', baseline_preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        # Baseline: TF-IDF only\n",
    "        baseline_score = cross_val_score(baseline_pipeline, X_text, y, cv=kf, scoring=f1_scorer).mean()\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"Baseline (TF-IDF only) F1-Macro: {baseline_score:.4f}\")\n",
    "\n",
    "        # Feature selection loop\n",
    "        best_score = baseline_score\n",
    "        selected_features = []\n",
    "\n",
    "        for feature in numerical_features:\n",
    "            if feature == \"speaking time (s)\":\n",
    "                continue\n",
    "\n",
    "            current_features = selected_features + [feature]\n",
    "\n",
    "            preprocessor = ColumnTransformer([\n",
    "                ('tfidf', tfidf, 'speech'),\n",
    "                ('num', StandardScaler(), current_features)\n",
    "            ])\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('clf', model)\n",
    "            ])\n",
    "\n",
    "            X_all = task_df[['speech'] + current_features]\n",
    "            new_score = cross_val_score(pipeline, X_all, y, cv=kf, scoring=f1_scorer).mean()\n",
    "\n",
    "            if new_score - best_score >= best_score * 0.001:\n",
    "                best_score = new_score\n",
    "                selected_features.append(feature)\n",
    "                print(f\"â Added Feature: {feature}, New F1-Macro: {new_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"â Skipped Feature: {feature}, F1-Macro: {new_score:.4f}\")\n",
    "\n",
    "        print(f\"\\nFinal Selected Features: {selected_features}, Final F1-Macro: {best_score:.4f}\")\n",
    "\n",
    "        # Final evaluation with all selected features\n",
    "        final_preprocessor = ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech'),\n",
    "            ('num', StandardScaler(), selected_features)\n",
    "        ]) if selected_features else ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech')\n",
    "        ])\n",
    "\n",
    "        final_pipeline = Pipeline([\n",
    "            ('preprocessor', final_preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        X_final = task_df[['speech'] + selected_features] if selected_features else task_df[['speech']]\n",
    "\n",
    "        scoring_final = {\n",
    "            'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "            'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "            'recall_macro': make_scorer(recall_score, average='macro')\n",
    "        }\n",
    "\n",
    "        final_scores = cross_validate(final_pipeline, X_final, y, cv=kf, scoring=scoring_final)\n",
    "\n",
    "        print(f\"\\nð Final Evaluation with Selected Features:\")\n",
    "        print(f\"F1-Macro: {final_scores['test_f1_macro'].mean():.4f} Â± {final_scores['test_f1_macro'].std():.4f}\")\n",
    "        print(f\"Precision-Macro: {final_scores['test_precision_macro'].mean():.4f}\")\n",
    "        print(f\"Recall-Macro: {final_scores['test_recall_macro'].mean():.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean F1 Macro\": final_scores['test_f1_macro'].mean(),\n",
    "            \"Std F1\": final_scores['test_f1_macro'].std(),\n",
    "            \"Mean Precision Macro\": final_scores['test_precision_macro'].mean(),\n",
    "            \"Mean Recall Macro\": final_scores['test_recall_macro'].mean(),\n",
    "            \"Type\": \"+\".join(selected_features) if selected_features else \"TF-IDF only\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv(\"/Users/kirillkonca/Documents/dementia_prediction/data.csv\")\n",
    "df = result\n",
    "\n",
    "# Define classification tasks\n",
    "tasks = {\n",
    "    \"MCI vs. AD\": df[df['diagnosis'].isin(['MCI', 'AD'])],\n",
    "    \"MCI vs. Control\": df[df['diagnosis'].isin(['MCI', 'Control'])],\n",
    "    \"AD vs. Control\": df[df['diagnosis'].isin(['AD', 'Control'])],\n",
    "    \"MCI vs. AD vs. Control\": df\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"SVM\": SVC(random_state=42, class_weight='balanced'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"mmse\")\n",
    "numerical_features.insert(0, \"mmse\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    print(f\"\\n### {task_name} ###\\n\")\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        X_text = task_df[['speech']]  # Keep as DataFrame\n",
    "        y = LabelEncoder().fit_transform(task_df['diagnosis'])\n",
    "\n",
    "        # Define baseline TF-IDF transformation\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, 1), max_features=5000, stop_words=stop_words)\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech')  # Start with only TF-IDF\n",
    "        ])\n",
    "\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "\n",
    "        # Baseline model with only TF-IDF\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        baseline_score = cross_val_score(pipeline, X_text, y, cv=kf, scoring=f1_scorer).mean()\n",
    "        print(f\"Baseline (TF-IDF only) F1-Macro: {baseline_score:.4f}\")\n",
    "\n",
    "        best_score = baseline_score\n",
    "        selected_features = []\n",
    "\n",
    "        # Try adding numerical features one by one\n",
    "        for feature in numerical_features:\n",
    "            if feature == \"speaking time (s)\":\n",
    "                continue\n",
    "\n",
    "            if len(selected_features) == 0:\n",
    "                f = [feature]\n",
    "            else:\n",
    "                f = selected_features + [feature]\n",
    "\n",
    "            preprocessor = ColumnTransformer([\n",
    "                ('tfidf', tfidf, 'speech'),\n",
    "                ('num', StandardScaler(), f)\n",
    "            ])\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('clf', model)\n",
    "            ])\n",
    "\n",
    "            new_score = cross_val_score(pipeline, task_df, y, cv=kf, scoring=f1_scorer).mean()\n",
    "\n",
    "            if new_score - best_score >= best_score * 0.001:  # Only keep features that improve performance\n",
    "                best_score = new_score\n",
    "                selected_features.append(feature)\n",
    "                print(f\"â Added Feature: {feature}, New F1-Macro: {new_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"â Skipped Feature: {feature}, F1-Macro: {new_score:.4f}\")\n",
    "\n",
    "        print(f\"\\nFinal Selected Features: {selected_features}, Final F1-Macro: {best_score:.4f}\")\n",
    "\n",
    "        # Rebuild final pipeline with selected features\n",
    "        final_preprocessor = ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech'),\n",
    "            ('num', StandardScaler(), selected_features)\n",
    "        ]) if selected_features else ColumnTransformer([\n",
    "            ('tfidf', tfidf, 'speech')\n",
    "        ])\n",
    "\n",
    "        final_pipeline = Pipeline([\n",
    "            ('preprocessor', final_preprocessor),\n",
    "            ('clf', model)\n",
    "        ])\n",
    "\n",
    "        # Prepare input for prediction\n",
    "        if selected_features:\n",
    "            X_all = task_df[['speech'] + selected_features]\n",
    "        else:\n",
    "            X_all = task_df[['speech']]\n",
    "\n",
    "        y_pred = cross_val_predict(final_pipeline, X_all, y, cv=kf)\n",
    "\n",
    "        precision = precision_score(y, y_pred, average='macro')\n",
    "        recall = recall_score(y, y_pred, average='macro')\n",
    "\n",
    "        print(f\"Precision (Macro): {precision:.4f}\")\n",
    "        print(f\"Recall (Macro): {recall:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean F1 Macro\": best_score,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"Type\": \"+\".join([feature for feature in selected_features])\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.T.to_dict().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"models_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "l0ZwS7poDJVv",
    "outputId": "aabacd60-b3e5-4998-d8b3-ebdf0b9550a1"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    data=results_df,\n",
    "    x=\"Task\",\n",
    "    y=\"Mean F1 Macro\",\n",
    "    hue=\"Model\",\n",
    "    capsize=0.1,\n",
    "    errwidth=1.5\n",
    ")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Best F1-Macro\")\n",
    "plt.xlabel(\"Classification Task\")\n",
    "plt.title(\"Model Performance Across Tasks\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.legend(title=\"Model\")\n",
    "\n",
    "# Add labels to bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X): \u001b[38;5;28;01mreturn\u001b[39;00m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys]  \u001b[38;5;66;03m# Keep multiple numerical columns as DataFrame\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/kirillkonca/Documents/dementia_prediction/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m X \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[43mnumerical_features\u001b[49m]  \u001b[38;5;66;03m# Speech (text) + Multiple numerical features\u001b[39;00m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagnosis\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Encode labels\u001b[39;00m\n\u001b[1;32m     25\u001b[0m feature_union \u001b[38;5;241m=\u001b[39m FeatureUnion([\n\u001b[1;32m     26\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m'\u001b[39m, Pipeline([\n\u001b[1;32m     27\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m, TextSelector(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m)), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ]))\n\u001b[1;32m     34\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numerical_features' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key): self.key = key\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X[self.key].values  # Convert to 1D array\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys): self.keys = keys\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X[self.keys]  # Keep multiple numerical columns as DataFrame\n",
    "\n",
    "df = pd.read_csv(\"/Users/kirillkonca/Documents/dementia_prediction/data.csv\")\n",
    "\n",
    "X = df[['speech'] + numerical_features]  # Speech (text) + Multiple numerical features\n",
    "y = LabelEncoder().fit_transform(df['diagnosis'])  # Encode labels\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('text_features', Pipeline([\n",
    "        ('selector', TextSelector('speech')), \n",
    "        ('tfidf', TfidfVectorizer(ngram_range=(1, 3), max_features=5000))\n",
    "    ])),\n",
    "    ('numerical_features', Pipeline([\n",
    "        ('selector', NumberSelector(numerical_features)), \n",
    "        ('scaler', StandardScaler())  # Scale all numerical features together\n",
    "    ]))\n",
    "])\n",
    "\n",
    "svm_model = SVC(kernel=\"sigmoid\", random_state=42, class_weight='balanced')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_union),  # Process text and numerical features separately\n",
    "    ('clf', svm_model)  # Classification model\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=kf, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['mms'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m excel_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kirillkonca/Downloads/Pitt/PItt-data.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract the 'id' and 'mms' columns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m excel_data \u001b[38;5;241m=\u001b[39m \u001b[43mexcel_file\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['mms'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file and select the \"data\" sheet\n",
    "excel_file = pd.read_excel('/Users/kirillkonca/Downloads/Pitt/PItt-data.xlsx', sheet_name='match')\n",
    "\n",
    "# Extract the 'id' and 'mms' columns\n",
    "excel_data = excel_file[['id', 'mms']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/kirillkonca/Documents/dementia_prediction/data.csv\")\n",
    "merged_df = pd.merge(df, excel_data, on='id', how='left')\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the \"match\" sheet from the Excel file\n",
    "excel_df = pd.read_excel('/Users/kirillkonca/Downloads/Pitt/PItt-data.xlsx', sheet_name='match')\n",
    "df = pd.read_csv('/Users/kirillkonca/Documents/dementia_prediction/data.csv')\n",
    "\n",
    "# Extract the main ID and sub-ID\n",
    "df['main_id'] = df['id'].apply(lambda x: int(x.split('-')[0]))  # Main ID as integer\n",
    "df['sub_id'] = df['id'].apply(lambda x: x.split('-')[1])  # Sub-ID as a string\n",
    "\n",
    "# Function to get the relevant mmse value\n",
    "def get_mmse(row):\n",
    "    main_id = row['main_id']\n",
    "    sub_id = row['sub_id']\n",
    "    mmse_column = f\"mmse{sub_id}\"\n",
    "    \n",
    "    # Check if the column exists in the \"match\" sheet\n",
    "    if mmse_column in excel_df.columns:\n",
    "        result = excel_df.loc[excel_df['id'] == main_id, mmse_column]\n",
    "        return result.values[0] if not result.empty else None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract mmse values\n",
    "df['mmse'] = df.apply(get_mmse, axis=1)\n",
    "\n",
    "# Drop helper columns if not needed\n",
    "df.drop(columns=['main_id', 'sub_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['mmse'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.merge(filtered_df, on=df.columns.tolist(), how='left', indicator=True)\n",
    "result = result[result['_merge'] == 'left_only'].drop(columns='_merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = df.dropna(subset=['diagnosis', 'mmse'])  # Drop rows where 'diagnosis' or 'mmse' is None\n",
    "\n",
    "tasks = {\n",
    "    \"MCI vs. AD\": df[df['diagnosis'].isin(['MCI', 'AD'])],\n",
    "    \"MCI vs. Control\": df[df['diagnosis'].isin(['MCI', 'Control'])],\n",
    "    \"AD vs. Control\": df[df['diagnosis'].isin(['AD', 'Control'])],\n",
    "    \"MCI vs. AD vs. Control\": df\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"SVM\": SVC(random_state=42, class_weight='balanced'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "f1_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for task_name, task_df in tasks.items():\n",
    "    print(f\"\\n### {task_name} ###\\n\")\n",
    "    X = task_df[['mmse']].values  # Use 'mmse' as the feature\n",
    "    y = LabelEncoder().fit_transform(task_df['diagnosis'])\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        scores = cross_val_score(model, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "        print(f\"F1-Macro: {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Model\": model_name,\n",
    "            \"Mean F1 Macro\": scores.mean(),\n",
    "            \"Std F1\": scores.std(),\n",
    "            \"Type\": \"mmse\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    data=results_df,\n",
    "    x=\"Task\",\n",
    "    y=\"Mean F1 Macro\",\n",
    "    hue=\"Model\",\n",
    "    capsize=0.1,\n",
    "    errwidth=1.5\n",
    ")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Best F1-Macro\")\n",
    "plt.xlabel(\"Classification Task\")\n",
    "plt.title(\"Model Performance Across Tasks (MMSE)\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.legend(title=\"Model\")\n",
    "\n",
    "# Add labels to bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result[result['diagnosis'] == 'Control'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"/Users/kirillkonca/Documents/dementia_prediction/data_pauses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>speech</th>\n",
       "      <th>annotation</th>\n",
       "      <th>speaking time (s)</th>\n",
       "      <th>on</th>\n",
       "      <th>co</th>\n",
       "      <th>short_pause</th>\n",
       "      <th>mid_pause</th>\n",
       "      <th>long_pause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138-1</td>\n",
       "      <td>Control</td>\n",
       "      <td>there's a cookie jar on the shelf . and the li...</td>\n",
       "      <td># sent_id = 1\\n# text = there's a cookie jar o...</td>\n",
       "      <td>46.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>631-0</td>\n",
       "      <td>Control</td>\n",
       "      <td>the kids are in the cookies . the stool is fal...</td>\n",
       "      <td># sent_id = 1\\n# text = the kids are in the co...</td>\n",
       "      <td>17.150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182-3</td>\n",
       "      <td>Control</td>\n",
       "      <td>Johnny's falling off the stool . the boy's fal...</td>\n",
       "      <td># sent_id = 1\\n# text = Johnny's falling off t...</td>\n",
       "      <td>17.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121-0</td>\n",
       "      <td>Control</td>\n",
       "      <td>the boy is taking a cookie out of the cookie j...</td>\n",
       "      <td># sent_id = 1\\n# text = the boy is taking a co...</td>\n",
       "      <td>128.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142-3</td>\n",
       "      <td>Control</td>\n",
       "      <td>the water's running over on the floor . the st...</td>\n",
       "      <td># sent_id = 1\\n# text = the water's running ov...</td>\n",
       "      <td>16.820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>270-1</td>\n",
       "      <td>AD</td>\n",
       "      <td>a boy is getting a cookie from the cookie jar ...</td>\n",
       "      <td># sent_id = 1\\n# text = a boy is getting a coo...</td>\n",
       "      <td>30.679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>213-2</td>\n",
       "      <td>AD</td>\n",
       "      <td>I see this woman here . and she's carrying som...</td>\n",
       "      <td># sent_id = 1\\n# text = I see this woman here ...</td>\n",
       "      <td>34.100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>091-1</td>\n",
       "      <td>AD</td>\n",
       "      <td>there's a boy getting in the cookie jar . and ...</td>\n",
       "      <td># sent_id = 1\\n# text = there's a boy getting ...</td>\n",
       "      <td>25.530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>579-0</td>\n",
       "      <td>AD</td>\n",
       "      <td>woman doing dishes . climbing up to get some c...</td>\n",
       "      <td># sent_id = 1\\n# text = woman doing dishes .\\n...</td>\n",
       "      <td>11.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>029-1</td>\n",
       "      <td>AD</td>\n",
       "      <td>I see the little boy stealing cookies from the...</td>\n",
       "      <td># sent_id = 1\\n# text = I see the little boy s...</td>\n",
       "      <td>101.529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id diagnosis                                             speech  \\\n",
       "0    138-1   Control  there's a cookie jar on the shelf . and the li...   \n",
       "1    631-0   Control  the kids are in the cookies . the stool is fal...   \n",
       "2    182-3   Control  Johnny's falling off the stool . the boy's fal...   \n",
       "3    121-0   Control  the boy is taking a cookie out of the cookie j...   \n",
       "4    142-3   Control  the water's running over on the floor . the st...   \n",
       "..     ...       ...                                                ...   \n",
       "535  270-1        AD  a boy is getting a cookie from the cookie jar ...   \n",
       "536  213-2        AD  I see this woman here . and she's carrying som...   \n",
       "537  091-1        AD  there's a boy getting in the cookie jar . and ...   \n",
       "538  579-0        AD  woman doing dishes . climbing up to get some c...   \n",
       "539  029-1        AD  I see the little boy stealing cookies from the...   \n",
       "\n",
       "                                            annotation  speaking time (s)  \\\n",
       "0    # sent_id = 1\\n# text = there's a cookie jar o...             46.200   \n",
       "1    # sent_id = 1\\n# text = the kids are in the co...             17.150   \n",
       "2    # sent_id = 1\\n# text = Johnny's falling off t...             17.800   \n",
       "3    # sent_id = 1\\n# text = the boy is taking a co...            128.050   \n",
       "4    # sent_id = 1\\n# text = the water's running ov...             16.820   \n",
       "..                                                 ...                ...   \n",
       "535  # sent_id = 1\\n# text = a boy is getting a coo...             30.679   \n",
       "536  # sent_id = 1\\n# text = I see this woman here ...             34.100   \n",
       "537  # sent_id = 1\\n# text = there's a boy getting ...             25.530   \n",
       "538  # sent_id = 1\\n# text = woman doing dishes .\\n...             11.810   \n",
       "539  # sent_id = 1\\n# text = I see the little boy s...            101.529   \n",
       "\n",
       "      on        co  short_pause  mid_pause  long_pause  \n",
       "0    0.0  0.105263            0          0           1  \n",
       "1    0.0  0.000000            0          0           0  \n",
       "2    0.0  0.076923            0          0           0  \n",
       "3    0.0  0.153846            0          1           0  \n",
       "4    0.0  0.285714            0          0           0  \n",
       "..   ...       ...          ...        ...         ...  \n",
       "535  0.0  0.214286            0          1           1  \n",
       "536  0.0  0.296296            1          1           0  \n",
       "537  0.0  0.052632            1          1           0  \n",
       "538  0.0  0.200000            0          1           0  \n",
       "539  0.0  0.250000            4          1           0  \n",
       "\n",
       "[540 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df and df2 are your dataframes, and both contain an 'id' column\n",
    "\n",
    "# Select only the relevant columns from df2\n",
    "pause_columns = ['id', 'short_pause', 'mid_pause', 'long_pause']\n",
    "\n",
    "# Merge into df based on 'id'\n",
    "result = result.merge(df2[pause_columns], on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"/Users/kirillkonca/Documents/dementia_prediction/data_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"data_mmse.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# We'll apply SFS on the output of preprocessing\n",
    "sfs = SequentialFeatureSelector(model, direction='forward', cv=kf, n_features_to_select=\"auto\", tol=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"mmse\")  # mmse will be added manually at the start\n",
    "numerical_features.insert(0, \"mmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(max_features=5000, stop_words=stop_words), 'speech'),\n",
    "            ('mmse', StandardScaler(), ['mmse'])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_sfs = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', sfs),\n",
    "    ('clf', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = result[result['diagnosis'].isin(['MCI', 'AD'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(max_features=5000, stop_words=stop_words), 'speech'),\n",
    "            ('mmse', StandardScaler(), ['mmse'])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mmse',\n",
       " 'speaking time (s)',\n",
       " 'on',\n",
       " 'co',\n",
       " 'mean_sent_embs',\n",
       " 'mlu',\n",
       " 'stop_words',\n",
       " 'tree_depth',\n",
       " 'verbs_with_inflections',\n",
       " 'nouns_with_determiners',\n",
       " 'sid',\n",
       " 'sid_efficiency',\n",
       " 'pid',\n",
       " 'pid_efficiency',\n",
       " 'maas',\n",
       " 'frazier_score',\n",
       " 'words_per_clause']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numerical_features + ['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = task[features]\n",
    "y = LabelEncoder().fit_transform(task['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated scores: [0.75454545 0.88979592 0.58966565 0.71458774 0.88979592 0.55978261\n",
      " 0.8125     0.75238095 0.79844961 0.75238095]\n",
      "Mean accuracy: 0.7513884808479798\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline_with_sfs, X, y, cv=kf, scoring=make_scorer(f1_score, average='macro'))\n",
    "\n",
    "print(\"Cross-validated scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/Users/kirillkonca/miniforge3/bin/python3 -m pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m numerical_features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      2\u001b[0m numerical_features\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove(\"mmse\")  # mmse will be added manually at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sequential Feature Selection on full training data...\n",
      "Selected 258 features out of 517\n",
      "\n",
      "=== Cross-Validation Results ===\n",
      "F1-macro scores: [nan nan nan nan nan nan nan nan nan nan]\n",
      "Average F1-macro: nan\n",
      "Standard deviation: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:978: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
      "    score = scorer._score(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
      "    y_pred = method_caller(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/Users/kirillkonca/miniforge3/lib/python3.9/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['AD' 'Control' 'MCI']\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "tfidf_max_features = 5000\n",
    "sfs_inner_folds = 5\n",
    "cv_outer_folds = 10\n",
    "random_seed = 42\n",
    "metric = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# ==== YOUR DATA ====\n",
    "# df: your DataFrame\n",
    "# numerical_features: list of additional numerical column names (excluding 'mmse')\n",
    "# target column name: 'target'\n",
    "\n",
    "# ==== STEP 1: TF-IDF Fit ====\n",
    "vectorizer = TfidfVectorizer(max_features=tfidf_max_features)\n",
    "X_tfidf = vectorizer.fit_transform(df['speech']).toarray()\n",
    "\n",
    "# ==== STEP 2: Prepare Extra Features ====\n",
    "extra_features = df[['mmse'] + numerical_features].reset_index(drop=True)\n",
    "scaler = StandardScaler()\n",
    "X_extra = scaler.fit_transform(extra_features)\n",
    "\n",
    "# ==== STEP 3: Combine TF-IDF + Extra Features ====\n",
    "X_full = np.hstack((X_tfidf, X_extra))\n",
    "y = df['diagnosis'].values\n",
    "\n",
    "# ==== STEP 4: One-time SFS on Entire Dataset ====\n",
    "print(\"Running Sequential Feature Selection on full training data...\")\n",
    "selector = SequentialFeatureSelector(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    n_features_to_select=\"auto\",\n",
    "    direction=\"forward\",\n",
    "    scoring='f1_macro',\n",
    "    cv=StratifiedKFold(n_splits=sfs_inner_folds, shuffle=True, random_state=random_seed),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "selector.fit(X_full, y)\n",
    "selected_features = selector.get_support()\n",
    "print(f\"Selected {np.sum(selected_features)} features out of {X_full.shape[1]}\")\n",
    "\n",
    "# ==== STEP 5: Cross-validate using selected features ====\n",
    "X_selected = X_full[:, selected_features]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "kfold = StratifiedKFold(n_splits=cv_outer_folds, shuffle=True, random_state=random_seed)\n",
    "scores = cross_val_score(clf, X_selected, y, scoring=metric, cv=kfold)\n",
    "\n",
    "# ==== FINAL RESULTS ====\n",
    "print(\"\\n=== Cross-Validation Results ===\")\n",
    "print(f\"F1-macro scores: {scores}\")\n",
    "print(f\"Average F1-macro: {scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m text_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m fixed_num_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmse\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Always included\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m candidate_num_features \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_features\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numerical_features' is not defined"
     ]
    }
   ],
   "source": [
    "text_feature = 'speech'\n",
    "fixed_num_features = ['mmse']  # Always included\n",
    "candidate_num_features = numerical_features  # Features for SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=5000, stop_words=stop_words), text_feature),\n",
    "        ('mmse', StandardScaler(), fixed_num_features),\n",
    "        ('sfs_features', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('sfs', SequentialFeatureSelector(\n",
    "                estimator=LogisticRegression(random_state=42, class_weight='balanced', max_iter=2000),\n",
    "                n_features_to_select='auto',  # Select top 2 features\n",
    "                direction='forward',\n",
    "                cv=cv_strategy,  # Use StratifiedKFold here\n",
    "                scoring=make_scorer(f1_score, average='macro'),\n",
    "                tol=0.05\n",
    "            ))\n",
    "        ]), candidate_num_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = result[result['diagnosis'].isin(['MCI', 'AD'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = task_df[['speech', 'mmse'] + numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(task_df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', LogisticRegression(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_validate(\n",
    "    estimator=pipeline,\n",
    "    X=X,  # Your full training DataFrame\n",
    "    y=y,\n",
    "    cv=cv_strategy,  # Stratified K-Fold\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,  # Optional: Compare train/test performance\n",
    "    n_jobs=-1  # Parallelize if possible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.95617104, 1.04173613, 0.94189072, 0.99636197, 1.04253221,\n",
       "        1.01032495, 1.03534317, 0.98465419, 0.59779501, 0.60730076]),\n",
       " 'score_time': array([0.01384091, 0.01288605, 0.01362705, 0.00716329, 0.00718307,\n",
       "        0.00678992, 0.00560284, 0.01393509, 0.00538301, 0.00527406]),\n",
       " 'test_f1_macro': array([0.75379939, 0.85326087, 0.88979592, 0.91793313, 0.8       ,\n",
       "        0.85326087, 0.87727273, 0.75238095, 0.79844961, 0.79844961]),\n",
       " 'train_f1_macro': array([0.85930172, 0.85930172, 0.86567901, 0.85930172, 0.85930172,\n",
       "        0.85621089, 0.85621089, 0.87228405, 0.86578215, 0.87228405]),\n",
       " 'test_precision_macro': array([0.72826087, 0.8       , 0.98      , 0.875     , 0.75      ,\n",
       "        0.85326087, 0.83333333, 0.71428571, 0.75      , 0.75      ]),\n",
       " 'train_precision_macro': array([0.80851064, 0.80851064, 0.81521739, 0.80851064, 0.80851064,\n",
       "        0.80434783, 0.80434783, 0.82222222, 0.81521739, 0.82222222]),\n",
       " 'test_recall_macro': array([0.79166667, 0.95833333, 0.83333333, 0.97916667, 0.9375    ,\n",
       "        0.85326087, 0.95652174, 0.91304348, 0.93478261, 0.93478261]),\n",
       " 'train_recall_macro': array([0.95734597, 0.95734597, 0.95971564, 0.95734597, 0.95734597,\n",
       "        0.95754717, 0.95754717, 0.96226415, 0.95990566, 0.96226415])}"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8294603084754015"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['tree_depth']\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X, y)\n",
    "\n",
    "# Extract selected features\n",
    "sfs = pipeline.named_steps['preprocessor'].named_transformers_['sfs_features'].named_steps['sfs']\n",
    "selected_indices = sfs.get_support(indices=True)\n",
    "selected_features = [candidate_num_features[i] for i in selected_indices]\n",
    "\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'speech', 'annotation', 'speaking time (s)', 'on',\n",
       "       'co', 'mean_sent_embs', 'mlu', 'stop_words', 'tree_depth',\n",
       "       'verbs_with_inflections', 'nouns_with_determiners', 'sid',\n",
       "       'sid_efficiency', 'pid', 'pid_efficiency', 'maas', 'frazier_score',\n",
       "       'words_per_clause', 'mmse', 'short_pause', 'mid_pause', 'long_pause'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
